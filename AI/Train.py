!pip install gensim sentencepiece scikit-learn tqdm

import json
import os
import sentencepiece as spm
from gensim.models import Word2Vec
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import requests
from tqdm import tqdm
import random

# â¬‡ï¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
def download_file(url, save_path):
    response = requests.get(url, stream=True)
    response.raise_for_status()
    with open(save_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    print(f"âœ… íŒŒì¼ ì €ì¥ë¨: {save_path}")

# âœ… ë°ì´í„° ë‹¤ìš´ë¡œë“œ
download_file('https://huggingface.co/datasets/Yuchan5386/SFT/resolve/main/data_shuffled_1.jsonl?download=true', 'faq_dataset.jsonl')
download_file('https://huggingface.co/Yuchan5386/inlam-100m/resolve/main/ko_unigram.model?download=true', 'spm.model')

# âœ… SentencePiece ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)
sp = spm.SentencePieceProcessor()
sp.load("spm.model")

# âœ… Word2Vec ëª¨ë¸ ì´ˆê¸°í™”
model = Word2Vec(vector_size=200, window=7, min_count=2, workers=4)

def generate_sentences(file_path, sample_rate=0.05):
    """JSONLì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì½ê³ , ì¼ì • í™•ë¥ ë¡œ ìƒ˜í”Œë§í•´ì„œ ë¬¸ì¥ í† í° ë°˜í™˜"""
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if random.random() > sample_rate:
                continue
            try:
                item = json.loads(line)
                conv = item.get("conversations", [])
                for turn in conv:
                    if turn["from"] == "human":
                        tokens = sp.encode_as_pieces(turn["value"])
                        yield tokens
            except:
                continue

# âœ… Word2Vec ì ì§„ í•™ìŠµ ì¤€ë¹„ (ë¦¬ìŠ¤íŠ¸ë¡œ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œ)
print("ğŸ“– Word2Vec í•™ìŠµìš© ë°ì´í„° ë¡œë”© ì¤‘...")
sentences = list(generate_sentences("faq_dataset.jsonl", sample_rate=0.02))  # ì „ì²´ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥

# âœ… ì–´íœ˜ ë¹Œë“œ (ì–´íœ˜ ì‚¬ì „ ìƒì„±)
print("ğŸ“– ì–´íœ˜ ë¹Œë“œ ì¤‘...")
model.build_vocab(sentences, progress_per=100000)
print("âœ… ì–´íœ˜ ìˆ˜:", len(model.wv))

# âœ… ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ ì ì§„ í•™ìŠµ
print("âš™ï¸ Word2Vec í•™ìŠµ ì¤‘...")
model.train(sentences, total_examples=model.corpus_count, epochs=3)
model.save("respiso.model")
print("âœ… Word2Vec í•™ìŠµ ì™„ë£Œ")

# âœ… FAQ ìƒ˜í”Œ ë°ì´í„° ë¡œë”© (limit ì œê±°)
def load_faq_subset(file_path):
    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            conv = item.get("conversations", [])
            if len(conv) >= 2:
                q = [c["value"] for c in conv if c["from"] == "human"]
                a = [c["value"] for c in conv if c["from"] == "gpt"]
                if q and a:
                    data.append({"question": q[0], "answer": a[0]})
    return data

faq_data = load_faq_subset("faq_dataset.jsonl")
print(f"âœ… FAQ ìƒ˜í”Œ {len(faq_data)}ê°œ ë¡œë“œ ì™„ë£Œ")

# âœ… ë¬¸ì¥ ë²¡í„° ê³„ì‚° í•¨ìˆ˜
def sentence_vector(sentence, model, sp):
    tokens = sp.encode_as_pieces(sentence)
    vecs = [model.wv[token] for token in tokens if token in model.wv]
    if len(vecs) == 0:
        return np.zeros(model.vector_size)
    return np.mean(vecs, axis=0)

faq_vectors = [sentence_vector(faq["question"], model, sp) for faq in faq_data]

# âœ… ì±—ë´‡ ì‘ë‹µ í•¨ìˆ˜
from sklearn.metrics.pairwise import cosine_similarity

def chatbot_response(user_input):
    user_vec = sentence_vector(user_input, model, sp).reshape(1, -1)
    sims = cosine_similarity(user_vec, faq_vectors)
    idx = sims.argmax()
    return faq_data[idx]["answer"]

# âœ… ì±—ë´‡ ì‹œì‘
print("ì±—ë´‡ ì‹œì‘! (ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ', 'exit', 'quit')")
while True:
    user_input = input("You: ")
    if user_input.lower() in ["ì¢…ë£Œ", "exit", "quit"]:
        print("ì±—ë´‡ ì¢…ë£Œ!")
        break
    print("Bot:", chatbot_response(user_input))
