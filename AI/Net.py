import json  
import numpy as np  
import pandas as pd
import tensorflow as tf  
from tensorflow.keras import layers 
import sentencepiece as spm  
import requests

# â¬‡ï¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
def download_file(url, save_path):
    response = requests.get(url, stream=True)
    response.raise_for_status()
    with open(save_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    print(f"âœ… íŒŒì¼ ì €ì¥ë¨: {save_path}")

# â¬‡ï¸ ë°ì´í„°ì™€ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
download_file('https://huggingface.co/datasets/Yuchan5386/TinyInst/resolve/main/ko_unigram.model?download=true', 'ko_unigram.model')
download_file('https://huggingface.co/datasets/Yuchan5386/TinyInst/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet?download=true', 'dataset.parquet')

# â¬‡ï¸ Parquet ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_parquet("dataset.parquet", engine="pyarrow")

# â¬‡ï¸ <start> ì§ˆë¬¸ <sep> ë‹µë³€ <end> í¬ë§·ìœ¼ë¡œ ë³€í™˜
train_sentences = []

for conversations in df["conversations"]:
    for i in range(0, len(conversations) - 1, 2):
        item1, item2 = conversations[i], conversations[i + 1]
        if item1.get("from") == "human" and item2.get("from") == "gpt":
            prompt = item1.get("value", "").strip().replace("\n", " ")
            response = item2.get("value", "").strip().replace("\n", " ")
            full = f"<start> {prompt} <sep> {response} <end>"
            train_sentences.append(full)
train_sentences = train_sentences
print(f"ì´ ë¬¸ì¥ ê°œìˆ˜: {len(train_sentences)}")

# â¬‡ï¸ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
sp = spm.SentencePieceProcessor()
sp.load("ko_unigram.model")

# â¬‡ï¸ íŠ¹ìˆ˜ í† í° ID ì¶”ì¶œ
pad_id = sp.piece_to_id("<pad>") if sp.piece_to_id("<pad>") != -1 else 0  
start_id = sp.piece_to_id("<start>")  
sep_id = sp.piece_to_id("<sep>")  
end_id = sp.piece_to_id("<end>")  
unk_id = sp.piece_to_id("<unk>")  

vocab_size = sp.get_piece_size()
print(f"âœ… Vocabulary size: {vocab_size}")

# â¬‡ï¸ í…ìŠ¤íŠ¸ <-> ID ë³€í™˜ í•¨ìˆ˜
def text_to_ids(text):
    return sp.encode(text, out_type=int)

def ids_to_text(ids):
    return sp.decode(ids)

# â¬‡ï¸ ì „ì²˜ë¦¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°
max_len = 100
batch_size = 128

# â¬‡ï¸ ì¸í’‹ê³¼ íƒ€ê²Ÿ ë§ˆìŠ¤í‚¹ í¬í•¨ëœ ì „ì²˜ë¦¬
encoded_inputs = []
targets = []

for sentence in train_sentences:
    if "<sep>" not in sentence:
        continue

    sep_index = sentence.index("<sep>")
    input_text = sentence[:sep_index + len("<sep>")].strip()
    target_text = sentence[sep_index + len("<sep>"):].strip()

    input_ids = text_to_ids(input_text)
    target_ids = text_to_ids(target_text + " <end>")

    full_input = input_ids + target_ids
    full_input = full_input[:max_len]

    target_mask = [0] * len(input_ids) + [1] * len(target_ids)
    target_mask = target_mask[:max_len]

    if len(full_input) < max_len:
        pad_len = max_len - len(full_input)
        full_input += [pad_id] * pad_len
        target_mask += [0] * pad_len

    encoded_inputs.append(full_input)

    target_seq = full_input[1:] + [end_id]
    target_seq = target_seq[:max_len]

    masked_target = [
        t if m == 1 else pad_id
        for t, m in zip(target_seq, target_mask)
    ]

    targets.append(masked_target)

# â¬‡ï¸ ë„˜íŒŒì´ ë³€í™˜
encoded_inputs = np.array(encoded_inputs)
targets = np.array(targets)

# â¬‡ï¸ TensorFlow Dataset ìƒì„±
def data_generator():
    for input_seq, target_seq in zip(encoded_inputs, targets):
        yield input_seq, target_seq

dataset = tf.data.Dataset.from_generator(
    data_generator,
    output_signature=(
        tf.TensorSpec(shape=(max_len,), dtype=tf.int32),
        tf.TensorSpec(shape=(max_len,), dtype=tf.int32)
    )
)

dataset = dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)

print("âœ… TF Dataset ìƒì„± ì™„ë£Œ!")

class Lo(layers.Layer):
    def __init__(self, d_model):
        super().__init__()
        # ë‚´ë¶€ ê³„ì‚°ì€ float32ë¡œ ìœ ì§€
        self.proj = layers.Dense(d_model, use_bias=True, dtype='float32')
        self.p = layers.Dense(128, use_bias=True, dtype='float32')
        self._out_dtype = 'float32'

    def call(self, x):
        # x may be bfloat16; cast to float32 for stable intermediate computation
        x_f32 = tf.cast(x, tf.float32)
        x = self.proj(x_f32)
        x = tf.nn.gelu(x)
        x = self.p(x)
        # cast back to model dtype for consistency
        return tf.cast(x, self._out_dtype)

class SwiRUCell(keras.layers.Layer):
    def __init__(self, units, **kwargs):
        self.units = units
        super(SwiRUCell, self).__init__(**kwargs)

    def build(self, input_shape):
        input_dim = input_shape[-1]

        # ğŸ”¸ ê¸°ë³¸ ê°€ì¤‘ì¹˜: ì…ë ¥ â†’ ê²Œì´íŠ¸
        self.W_ih = self.add_weight(
            shape=(input_dim, self.units),
            initializer='glorot_uniform',
            name='W_ih',
            trainable=True
        )
        self.b_h = self.add_weight(
            shape=(self.units,),
            initializer='zeros',
            name='b_h',
            trainable=True
        )

        # ğŸ”¸ Dense ë ˆì´ì–´ëŠ” __init__ì—ì„œ ìƒì„±í•´ì•¼ í•˜ì§€ë§Œ, build ì‹œì ì— input_dim ì•Œ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ìƒì„± ê°€ëŠ¥
        # ê·¸ëŸ¬ë‚˜ Keras í˜¸í™˜ì„±ì„ ìœ„í•´ __init__ì—ì„œ ìƒì„±í•˜ëŠ” ê²ƒì´ ë” ì•ˆì „ â†’ ì´ë¥¼ ìœ„í•´ get_configë„ êµ¬í˜„ í•„ìš”
        # ê°„ë‹¨íˆ í•˜ê¸° ìœ„í•´ ì—¬ê¸°ì„œ ìƒì„±í•˜ë˜, build í˜¸ì¶œ ë³´ì¥ ì „ì œ
        self.W1 = layers.Dense(self.units, activation='silu', name='W1')
        self.W2 = layers.Dense(self.units, name='W2')  # no activation
        self.ln = layers.LayerNormalization(epsilon=1e-5, dtype="float32", name='ln')

        # ğŸ”¸ ì„œë¸Œ ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ build
        self.W1.build(input_shape)
        self.W2.build(input_shape)
        self.ln.build((None, self.units))

        self.built = True

    def call(self, inputs, states):
        h_prev = states[0]
        # ê²Œì´íŠ¸ f âˆˆ [0,1]
        f = tf.sigmoid(tf.matmul(inputs, self.W_ih) + self.b_h)
        # SwiGLU ìŠ¤íƒ€ì¼: Swish(x) âŠ™ x
        x = self.W1(inputs) * self.W2(inputs)
        # RRU ìŠ¤íƒ€ì¼ ì—…ë°ì´íŠ¸ + LayerNorm + residual
        h_raw = f * h_prev + (1.0 - f) * x
        h_norm = self.ln(h_raw)
        h = h_norm + h_prev  # residual connection
        return h, [h]

    @property
    def state_size(self):
        return self.units

    @property
    def output_size(self):
        return self.units

    def get_config(self):
        config = super().get_config()
        config.update({"units": self.units})
        return config

class Respiso(tf.keras.Model):
    def __init__(self, vocab_size, max_seq_len, d_model, n_layers, dropout_rate=0.1):
        super().__init__()
        self.token_embedding = layers.Embedding(vocab_size, d_model)
        self.rnn_cell = SwiRUCell(units=d_model)
        self.rnn_layer = tf.keras.layers.RNN(
            self.rnn_cell,
            return_sequences=True,
            return_state=False
        )
        self.initial_state_trainable = self.add_weight(
            shape=(1, rnn_units),
            initializer='zeros',
            trainable=True,
            name='initial_hidden_state'
        )
        # LayerNormalizationì€ float32ë¡œ í•´ì„œ ì •ë°€ë„ ë¬¸ì œ ë°©ì§€
        self.ln_f = layers.LayerNormalization(epsilon=1e-5, dtype="float32")

    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        x = self.token_embedding(x)
        # ë°°ì¹˜ í¬ê¸°ë§Œí¼ initial_state ë³µì œ
        initial_state = tf.tile(self.initial_state_trainable, [batch_size, 1])
        x = self.rnn_layer(x, initial_state=initial_state)

        x = self.ln_f(x)

        embedding_matrix = tf.cast(self.token_embedding.embeddings, x.dtype)
        logits = tf.matmul(x, embedding_matrix, transpose_b=True)
        return tf.cast(logits, tf.float32)

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')

def masked_loss(y_true, y_pred):
    loss = loss_fn(y_true, y_pred)
    mask = tf.cast(tf.not_equal(y_true, pad_id), tf.float32)
    masked_loss = tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)
    return masked_loss

def masked_perplexity(y_true, y_pred):
    loss = loss_fn(y_true, y_pred)
    mask = tf.cast(tf.not_equal(y_true, pad_id), tf.float32)
    avg_loss = tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)
    return tf.exp(tf.minimum(avg_loss, 10.0))  # ìˆ˜ì¹˜ ì•ˆì •ì„± í™•ë³´

def create_lr_schedule(initial_lr=5e-5, decay_steps=10000, decay_rate=0.9):
    return tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=initial_lr,
        decay_steps=decay_steps,
        decay_rate=decay_rate,
        staircase=False
    )

# ëª¨ë¸ ìƒì„±
model = Respiso(
    vocab_size=vocab_size,
    max_seq_len=max_len,
    d_model=256,
    n_layers=1
)

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
optimizer = tf.keras.optimizers.Adam(
    learning_rate=create_lr_schedule(),
    beta_1=0.9,
    beta_2=0.95,
    epsilon=1e-8,
    clipnorm=1.0
)

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(
    optimizer=optimizer,
    loss=masked_loss,
    metrics=[
        masked_accuracy,
        masked_perplexity
    ]
)

# ë”ë¯¸ ì¸í’‹ìœ¼ë¡œ ëª¨ë¸ ì´ˆê¸°í™”
dummy_input = np.zeros((1, max_len), dtype=np.int32)
model(dummy_input)
model.summary()

# í•™ìŠµ ì‹œì‘
history = model.fit(
    dataset,
    epochs=1,
    steps_per_epoch = encoded_inputs.shape[0] // batch_size,
    verbose=1
)

# ê°€ì¤‘ì¹˜ ì €ì¥
model.save_weights("Cobra.weights.h5")
print("ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ!")

def generate_text_topp(model, prompt, max_len=100, max_gen=98, p=0.9, temperature=0.8, min_len=20):
    model_input = text_to_ids(f"<start> {prompt} <sep>")
    model_input = model_input[:max_len]
    generated = list(model_input)
    for step in range(max_gen):
        if len(generated) > max_len:
            input_seq = generated[-max_len:]
        else:
            input_seq = generated
        input_padded = np.pad(input_seq, (0, max_len - len(input_seq)), constant_values=pad_id)
        input_tensor = tf.convert_to_tensor([input_padded])
        logits = model(input_tensor, training=False)
        next_token_logits = logits[0, len(input_seq) - 1].numpy()
        next_token_logits[end_id] -= 5.0
        next_token_logits[pad_id] -= 10.0
        probs = tf.nn.softmax(next_token_logits / temperature).numpy()
        sorted_indices = np.argsort(probs)[::-1]
        sorted_probs = probs[sorted_indices]
        cumulative_probs = np.cumsum(sorted_probs)
        cutoff = np.searchsorted(cumulative_probs, p)
        top_indices = sorted_indices[:cutoff + 1]
        top_probs = sorted_probs[:cutoff + 1]
        top_probs /= np.sum(top_probs)
        next_token_id = np.random.choice(top_indices, p=top_probs)
        if next_token_id == end_id and len(generated) >= min_len:
            break
        generated.append(int(next_token_id))
    return ids_to_text(generated)

print("\n\n===== ìƒì„± ê²°ê³¼ =====")  
print(generate_text_topp(model, "ì•ˆë…•", p=0.9))
